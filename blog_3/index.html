<!DOCTYPE html>
<html>
<head>

  <title>Blog Post 3</title>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="author" content=""> <!-- Your name here -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
  <!--# CSS #}-->
  <link rel="stylesheet" href="style.css"> <!-- Add your CSS to this file -->

  <!--# JAVASCRIPT #}-->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js"></script> <!-- JQUERY -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.16/d3.min.js"></script> <!-- D3.js -->
  <script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
</head>

<body>

<div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">
            <div class="page-header center">
                <h1>Blog Post 3: <small>&laquo;Reddit Analysis&raquo;</small></h1>
            </div>

      <h2>Members</h2>
      The members of this group are mjzhu, bp12, eh49, and amai.

      <h2>What We Did This Week</h2>
      <h3>Spark</h3>
      For spark, we deleted our old single slave cluster and created a cluster with 4 slaves. We were able to get spark jobs running on the cluster that use http calls to get comments from Solr and perform simple word counts on the comments. In order to get our Spark jobs running within the job-server, we need to add it as a dependency to the sbt project containing the Spark jobs then modify the Spark jobs to implement the job-server interface. This modification will be minimal and should not take very much work.

      <h3>Solr</h3>
      We are still in the process of uploading all the Reddit comment dataset. We ran into a problem where we could not download the Solr data efficiently, but we believe this is due to our instance's micro size. We will upgrade our instance to large to see if this increases download speed. 
      
      <h3>Flask</h3>
      We have created more visualizations using D3 and have facilitated a connection from the Flask app to the Spark job server.
      We created a sample visualization that uses stored data in order to display the relative size of subreddits to the user. More impressively, we set up a visualization that allows a user to dynamically choose a word or phrase, and we graph the usage of that word or phrase over time. This visualization calls a flask handler that directly queries Solr and gets the number of instances during each time period.
      Here is a screen shot of the word "cool"
      <img src="cool.png">
      Here is another one of a less popular word
      <img src="dank.png">
      This is the raw number of comments we found with that word in it. Remember, we sampled about 1/5 of reddit comments, so these values are relative. Also, reddit is rapidly growing, so each word will naturally increase in usage. We plan to add a visualization that shows the relative increase in popularity of a word. You can play with our current webapp <a href="http://ec2-54-175-87-193.compute-1.amazonaws.com/">here</a>.

      <h3>Machine Learning</h3>
      We have implemented a few machine learning algorithms for Spark, and we have to connect them with Spark by figuring out how to run a Spark job. One example algorithm is the Flesch-Kinkaid reading level algorithm, for which we have code written in Scala to integrate with Spark, and the code can calculate the number of syllables within each Reddit comment, along with number of sentences and words. In addition, we are looking over how to use Spark's built-in LDA (Latent Dirichlet allocation) algorithm for analyzing topic models related to our Reddit comments. We are also working on writing a few others like sentiment analysis that we can plot in our visualizations graph.

        <div class="col-md-2"></div>
    </div>  

</body>

</html>
